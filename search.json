[
  {
    "objectID": "posts/openMM/cosmo_tutorial.html",
    "href": "posts/openMM/cosmo_tutorial.html",
    "title": "COSMO tutorial",
    "section": "",
    "text": "This tutorial guide you how to use cosmo package to perform coarse-grained simulations of IDPs\n\n\nInstallation Cosmo\nFirst, you need to follow the Readme file here to install the package.\n\n\nGenerate initial structure:\nTo generate initial structure, we can use a provided script: scripts/generate_structure_from_fasta/generate_structure.py\nIn this script, there are some parameters need to be changed:\nseq = 'MRTQWPSPAKLNLFLYITGQRADGYHTLQTLFQFLDYGDTISIELRDDGDIRLLTPVEGVEHEDNLIVRAARLLMKTAADSGRLPTGSGANISIDKRLPMGGGLGGGSSNAATVLVALNHLWQCGLSMDELAEMGLTLGADVPVFVRGHAAFAEGVGEILTPVDPPEKWYLVAHPGVSIPTPVIFKDPELPRNTPKRSIETLLKCEFSNDCEVIARKRFREVDAVLSWLLEYAPSRLTGTGACVFAEFDTESEARQVLEQAPEWLNGFVAKGANLSPLHRAML'\nprotein_name = \"2ww4\"\n# number of chains = nx*ny\nnx = 2\nny = 2\nThe long axis of molecule will be aligned along z-axis, we will expand the system along x-y plane.\n\nline 1: the fasta sequence of protein we are studying\nline 2: name of protein, will be used to write pdb output file name\nlines 3-4: number of copies in x-y dimension, in case simulation of multichains\n\n\n\nSimulation control file:\nSimulation control parameters are saved in *ini file format (python file format): here, I will named it as md.ini\n[OPTIONS]\nmd_steps = 10_000 # number of steps\ndt = 0.01 ; time step in ps\nnstxout = 100 ; number of steps to write checkpoint = nstxout\nnstlog = 100 ; number of steps to print log\nnstcomm = 100 ; frequency for center of mass motion removal\n; select HPS model, available options: hps_kr, hps_urry, hps_ss, or mpipi\nmodel = mpipi\n\n; control temperature coupling\ntcoupl = yes\nref_t = 310 ; Kelvin- reference temperature\ntau_t = 0.01 ; ps^-1\n\n;pressure coupling\npcoupl = no\nref_p = 1\nfrequency_p = 25\n\n; Periodic boundary condition: if pcoupl is yes then pbc must be yes.\npbc = yes\n; if pbc=yes, then use box_dimension option to specify box_dimension = x or [x, y, z], unit of nanometer\n; otherwise, box_dimension is not required and does not take any effect.\nbox_dimension = 30 ; [30, 30, 60]\n\n; input\nprotein_code = ASYN\npdb_file = asyn.pdb\n; output\ncheckpoint = asyn.chk\n;Use GPU/CPU\ndevice = GPU\n; If CPU is specified, then use ppn variable\nppn = 4\n;Restart simulation\nrestart = no\nminimize = yes ;if not restart, then minimize will be loaded, otherwise, minimize=False\nThis control file is almost self-explaination if you are familiar with other MD simulation packages.\n\n\nRunning simulation\nTo run simulation, there are two options:\n\nNormal canonical simulation using Langevin dynamics: hps-simulation -f md.ini\nIf you want to customize your simulation to use some advanced methods, make a copy of examples/standard_example/run_simulation.py Customize the content of this script then execute the command: python run_simulation -f md.ini\n\n\n\nAnalyze simulation trajectory\nSimulation will generate a psf file for topology and dcd file for trajectory, in addition, a log file that contains potential energy, kinetics energy, total energy and time remain to run simulations. But mainly you will need to analyze the trajectory to extract the information you need.\nIn the scripts/ directory, I provided some scripts I used in my study but you may need to develop for yours."
  },
  {
    "objectID": "posts/openMM/openmm_code_snippet.html",
    "href": "posts/openMM/openmm_code_snippet.html",
    "title": "Simple OpenMM system for testing code development",
    "section": "",
    "text": "This post contains a simple system definition of two beads with custom nonbonded for testing\n\nimport openmm as mm\nimport openmm.app as app\nimport numpy as np\n\nprint(mm.__version__)\nsys = mm.System()\n# 2 GLY\nsys.addParticle(57.05*mm.unit.amu)\nsys.addParticle(57.05*mm.unit.amu)\n\n# build topology\ntop=mm.app.topology.Topology()\ntop.addChain('prot')\nprot_chain = list(top.chains())[-1]\ntop.addResidue('GLY', prot_chain)\nres1 = list(top.residues())[-1]\ntop.addAtom('CA', app.Element.getBySymbol('C'), res1)\n\ntop.addResidue('GLY', prot_chain)\nres2 = list(top.residues())[-1]\ntop.addAtom('CA', app.Element.getBySymbol('C'), res2)\n\n# # first implementation\n# energy_function = '((rc/r)^(4)-1)^(2)'\n# wang_frenkel_force = mm.CustomNonbondedForce(energy_function)\n# wang_frenkel_force.addGlobalParameter('rc', 1.408533*mm.unit.nanometer)\n\n# second implementation\nenergy_function = 'step(rc-r)*((rc/r)^(2*mu)-1)^(2*nu);'\nwang_frenkel_force = mm.CustomNonbondedForce(energy_function)\nwang_frenkel_force.addGlobalParameter('rc', 1.408533*mm.unit.nanometer)\nwang_frenkel_force.addGlobalParameter('mu', 2)\nwang_frenkel_force.addGlobalParameter('nu', 1)\n\n# # # third implementation\n# energy_function = '((rc/r)^(4)-1)^(2*nu);'\n# wang_frenkel_force = mm.CustomNonbondedForce(energy_function)\n# wang_frenkel_force.addGlobalParameter('rc', 1.408533*mm.unit.nanometer)\n# # wang_frenkel_force.addGlobalParameter('mu', 2)\n# wang_frenkel_force.addGlobalParameter('nu', 1)\n\nfor _ in top.atoms():\n    wang_frenkel_force.addParticle(())\n\nsys.addForce(wang_frenkel_force)\n\nintegrator = mm.LangevinIntegrator(1*mm.unit.kelvin, 1/mm.unit.picosecond, 0.000001*mm.unit.picosecond)\n\n# # CPU\n# platform = mm.Platform.getPlatformByName('CPU')\n# properties = {'Threads': str(8)}\n\n# GPU\nplatform = mm.Platform.getPlatformByName('CUDA')\nproperties = {'CudaPrecision': 'single', \"DeviceIndex\": \"0\"}\nsim = mm.app.Simulation(top, sys, integrator, platform, properties)\n\n# pos=np.array([[0,0,0],[1.409,0,0]])\n# pos=np.array([[0,0,0],[1.4,0,0]])\npos=np.array([[0,0,0],[1.9,0,0]])\n\nsim.context.setPositions(pos)\nstate = sim.context.getState(getForces=True, getEnergy=True)\nprint(platform)\nprint(properties)\nprint(state.getPotentialEnergy())\nprint(state.getForces())"
  },
  {
    "objectID": "posts/Amino-acid-table/index.html",
    "href": "posts/Amino-acid-table/index.html",
    "title": "Amino acid table",
    "section": "",
    "text": "Amino acids table with their properties\n21 amino acids make up proteins.\n\n(Source: Wikipedia)"
  },
  {
    "objectID": "posts/2023/October/download_PED_entries.html",
    "href": "posts/2023/October/download_PED_entries.html",
    "title": "Download Protein Ensemble Database entries",
    "section": "",
    "text": "The Protein Ensemble Database (PED) is a freely accessible repository designed for the submission of structural collections, which include intrinsically disordered proteins (IDPs). PED contains manually curated records of structural collections measured through techniques such as nuclear magnetic resonance spectroscopy, small-angle X-ray scattering, and fluorescence resonance energy transfer. These structural coordinates can be utilized to assess these collections, thereby aiding the development of novel modeling approaches aimed at enhancing our ability to establish links between the inherent “absence of a fixed structure” in IDPs and their functions. Each PED entry corresponds to the primary experimental data and the structural collections associated with these datasets."
  },
  {
    "objectID": "posts/2023/October/download_PED_entries.html#separate-chains-and-fix-pdb-if-necessary",
    "href": "posts/2023/October/download_PED_entries.html#separate-chains-and-fix-pdb-if-necessary",
    "title": "Download Protein Ensemble Database entries",
    "section": "Separate chains and fix PDB if necessary",
    "text": "Separate chains and fix PDB if necessary\nFor the entry with multiple chains, before we can perform entanglement analysis, we need additional step is seperate individual chain.\n\ndef separate_chains(original_pdb, output_folder):\n    \"\"\"\n    Separate chains from a Protein Data Bank (PDB) file and save them as individual files.\n\n    Parameters:\n    - original_pdb (str): The path to the original PDB file to process.\n    - output_folder (str): The folder where the separated chain files will be saved.\n\n    Returns:\n    - None\n\n    This function reads an original PDB file and separates its contents into individual chain-specific\n    PDB files. Each chain-specific file contains only the alpha carbon (CA) atom records for a particular chain.\n\n    If the specified output folder does not exist, it will be created.\n\n    Example Usage:\n    separate_chains(\"input.pdb\", \"output_folder/\")\n    \"\"\"\n    # Extract the base name without the file extension\n    base_name = os.path.splitext(os.path.basename(original_pdb))[0]\n\n    # Check if the output folder exists, and create it if it doesn't\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    # Read content of the original PDB file\n    with open(original_pdb, 'r') as f:\n        content = f.readlines()\n\n    # Collect unique chain identifiers and alternate location indicators\n    chain_identifiers = set()\n    alternate_location_indicators = set()\n\n    for line in content:\n        if line.startswith(\"ATOM\"):\n            chain_identifiers.add(line[21:22])\n            alternate_location_indicators.add(line[16:17])\n\n    # Create separate PDB files for each chain\n    for chain_identifier in chain_identifiers:\n        output_file = os.path.join(output_folder, f\"{base_name}_{chain_identifier}.pdb\")\n\n        with open(output_file, 'w') as f:\n            for line in content:\n                if line.startswith(\"MODEL\") or line.startswith(\"ENDMDL\"):\n                    # Write entire MODEL and ENDMDL lines\n                    f.write(line)\n                elif line.startswith(\"ATOM\"):\n                    atom_name = line[12:16].strip()\n                    current_chain = line[21:22]\n                    alternate_location = line[16:17]\n\n                    # Check if the line matches the criteria for the current chain\n                    if atom_name == \"CA\" and current_chain == chain_identifier and alternate_location in alternate_location_indicators:\n                        f.write(line)\n\n\n# separate_chains('pdb_multiple/PED00226e001.pdb', 'fixed_multiple')\n# Example Usage:\nseparate_chains('pdb_multiple/PED00226e001.pdb', \"output_folder/\")"
  },
  {
    "objectID": "posts/Create-env-for-openmm-cg-simulations/Openmm-coarse-grained-simulations.html",
    "href": "posts/Create-env-for-openmm-cg-simulations/Openmm-coarse-grained-simulations.html",
    "title": "Openmm Coarse-grained Simulations",
    "section": "",
    "text": "#!/bin/bash\n#SBATCH --partition plgrid\n#SBATCH --job-name cryosparc-master\n#SBATCH --nodes 1\n#SBATCH --ntasks-per-node 1\n#SBATCH --mem 15GB\n#SBATCH --time 72:00:00\n#SBATCH -C localfs\n#SBATCH -A plgrisa-cpu\n#SBATCH --dependency=singleton\n#SBATCH --output cryosparc-master-log-%J.txt\n#SBATCH --signal=B:2@240\n\necho \"Job run\" &gt; test.txt"
  },
  {
    "objectID": "posts/Create-env-for-openmm-cg-simulations/Openmm-coarse-grained-simulations.html#install-packages",
    "href": "posts/Create-env-for-openmm-cg-simulations/Openmm-coarse-grained-simulations.html#install-packages",
    "title": "Openmm Coarse-grained Simulations",
    "section": "Install packages",
    "text": "Install packages\nconda install -c openmm=7.7 parmed mdtraj\nNotes on cluster information:\n* local machine: (py310) openMM 7.7+ cudatoolkit 11.6\n* Plgrid: (py310) openMM 7.7+ cudatoolkit 10.2\n* ACI: (base) OpenMM 7.7+ cudatoolkit 11.7\n* Ares: openMM 7.7 + cudatoolkit 11.6 ( GPU Tesla V100)\n\nJob with GPU\n#!/bin/bash\n#SBATCH --job-name ares_gpu\n#SBATCH --nodes 1\n#SBATCH --partition plgrid-gpu-v100\n#SBATCH --gres=gpu:1\n#SBATCH --ntasks-per-node 1\n#SBATCH --mem 15GB\n#SBATCH --time 48:00:00\n#SBATCH -C localfs\n#SBATCH -A plgrisa-gpu\n#SBATCH --dependency=singleton\n#SBATCH --output cryosparc-master-log-%J.txt\n#SBATCH --signal=B:2@240\n\n## prometheus\n#source /net/people/plgqvuvan/anaconda3/etc/profile.d/conda.sh\n## ares\n\ncd $SLURM_SUBMIT_DIR\n\nconda init bash\nsource /net/people/plgrid/plgqvuvan/plggligroup/qvv5013/anaconda3/etc/profile.d/conda.sh\nconda activate py310\nmodule add cuda/11.6.0\necho \"NVIDIA-DRIVER version:\"`nvidia-smi`\n\npython single_run_extend.py -f control.cntrl"
  },
  {
    "objectID": "posts/Create-env-for-openmm-cg-simulations/Openmm-coarse-grained-simulations.html#interactive-mode-on-ares",
    "href": "posts/Create-env-for-openmm-cg-simulations/Openmm-coarse-grained-simulations.html#interactive-mode-on-ares",
    "title": "Openmm Coarse-grained Simulations",
    "section": "interactive mode on ares:",
    "text": "interactive mode on ares:\n\nusing GPU: srun -p plgrid-gpu-v100 --nodes=1 --ntasks=1 --mem=5GB --time=0-1 --pty bash\nusing CPU: srun -p plgrid --nodes=1 --ntasks=1 --mem=5GB --time=0-1 --pty bash"
  },
  {
    "objectID": "posts/python/Beautiful-plot-with-Matplotlib.html",
    "href": "posts/python/Beautiful-plot-with-Matplotlib.html",
    "title": "Make a beautiful plot using Matplotlib",
    "section": "",
    "text": "When writing a paper, a beautiful figure is very important. Before, I used Gnuplot or XMGRACE to make figures because it is easy to config.\nRecently I moved to Matplotlib and used it in many of my projects, found it convenient to make plots and tweak them to produce beautiful figures.\nHere are some snippet for reference to easily look back."
  },
  {
    "objectID": "posts/python/Beautiful-plot-with-Matplotlib.html#import-library",
    "href": "posts/python/Beautiful-plot-with-Matplotlib.html#import-library",
    "title": "Make a beautiful plot using Matplotlib",
    "section": "Import library",
    "text": "Import library\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib notebook\n\nConfig for matplotlib\n\n# config for matplotlib\nfont = {'family' : 'arial',\n        'weight' : 'normal',\n        'size'   : 12}\n\nmatplotlib.rc('font', **font)\nmatplotlib.rcParams['axes.spines.right'] = False\nmatplotlib.rcParams['axes.spines.top'] = False\nmatplotlib.rcParams['axes.linewidth'] = 2 #set the value globally\n\n# set tick width\nmatplotlib.rcParams['xtick.major.size'] = 6\nmatplotlib.rcParams['xtick.major.width'] = 2\nmatplotlib.rcParams['ytick.major.size'] = 6\nmatplotlib.rcParams['ytick.major.width'] = 2\n\n\n# Create a fake data:\nx = np.arange(0, 2*np.pi, 2*np.pi/1000)\ny = np.sin(x)\ny2 = np.cos(x)\n\n\nfig = plt.figure(figsize=(8,5))\nplt.plot(x,y, color='black', lw=2, label='sin(x)', alpha=0.8)\nplt.plot(x,y2, color='red', lw=2, label='cos(x)', alpha=0.8)\n\n# Plot horizontal\nplt.axhline(y = 0.0, color = 'blue', linestyle = '--', alpha=0.5)\n\n# Config Legends and labels\n# set yticks\nplt.yticks([-1, 0, 1], ['first', 'second', ''])\n\nplt.legend(loc='upper center', ncol=2)\nplt.xlim(0, 2*np.pi)\nplt.ylim(-1,1)\nplt.ylabel(r'$f_x$') #latex type\nplt.xlabel('x')\n\n# To save figure with high DPI, uncomment following line: the more dpi, the clearer figure but large file\n# plt.savefig('Demo_matplotlib.png', dpi=1200)\n\n\n\n\n\n\n\nText(0.5, 0, 'x')"
  },
  {
    "objectID": "pages/news/index.html",
    "href": "pages/news/index.html",
    "title": "News",
    "section": "",
    "text": "Dates\n\n\nEvents\n\n\n\n\n\n\n01/09/2023\n\n\nQuyen Vu has successfully passed the Physics Exam at IFPAN with a score of 4.5/5. This marks the final examination before he defends his PhD thesis. The exam covered General Physics and profile-specific themes.\n\n\n\n\n06/07/2023\n\n\nQuyen Vu delivered a brief presentation about his PhD thesis in front of the Scientific Council of IFPAN.\n\n\n\n\n22/06/2023\n\n\nQuyen Vu has submitted his PhD thesis."
  },
  {
    "objectID": "pages/tools/index.html",
    "href": "pages/tools/index.html",
    "title": "Tools",
    "section": "",
    "text": "COSMO:\nCOarse-grained Simulation of intrinsically disordered prOteins with openMM. Package utilize the customizable, high-performance of OpenMM and good science from high-quality research.\nCurrently, there are four models are supported:\n\nhps_urry: Hydropathy according to Urry scale (default, Recommended).\nhps_kr: Kapcha-Rossy scale. This model has parameters for nucleic acids and post-translational modification residues.\nhps_ss: hps_urry with bonded potential.\nmpipi: another model that using Wang-Frenkel short range potential instead of LJ 12-6\nOther models can be easily implemented by defining them in cosmo/parameters/model_parameters.py\n\nThe package is ready for studying various problems such as, conformation dynamics of single chain, LLPS …\n(Source code is available on Github)\nCheck out docs for more details here\n\n\nEntanglement Analysis:\nThere are two main version of entanglement analysis:\n\nThe first one is characterized by the linking number of loops and the open terminal, which is composed of the whole terminal. Please check this paper for more details. This kind of entanglement can be calculated by the following code which is developed by Ian Sitarik (Penn State). Python version\nThe maximum linking number between the loop and all possible segments of the terminal is calculated by the following code: Julia version. There are two versions for this kind of entanglement calculation, in python and Julia. julia is much faster than Python."
  },
  {
    "objectID": "pages/publications/index.html",
    "href": "pages/publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Halder, R.; Nissley, D. A.; Sitarik, I.; Jiang, Y.; Rao, Y.; Vu, Q. V; Li, M. S.; Pritchard, J.; O’Brien, E. P. How Soluble Misfolded Proteins Bypass Chaperones at the Molecular Level. Nat. Commun. 2023, 14 (1), 3689. Read more\nVu, Q. V; Nissley, D. A.; Jiang, Y.; O’Brien, E. P.; Li, M. S. Is Posttranslational Folding More Efficient Than Refolding from a Denatured State: A Computational Study. J. Phys. Chem. B 2023, 127 (21), 4761–4774. Read more\nVu, Q. V; Sitarik, I.; Jiang, Y.; Yadav, D.; Sharma, P.; Fried, S. D.; Li, M. S.; O’Brien, E. P. A Newly Identified Class of Protein Misfolding in All-atom Folding Simulations Consistent with Limited Proteolysis Mass Spectrometry. (In Review). PrePrint available\nLeininger, S. E.; Rodriguez, J.; Vu, Q. V.; Jiang, Y.; Li, M. S.; Deutsch, C.; O’Brien, E. P. Ribosome Elongation Kinetics of Consecutively Charged Residues Are Coupled to Electrostatic Force. Biochemistry 2021, 60, 43, 3223–3235. Read more\nVu, Q. V.; Jiang, Y.; Li, M. S.; O’Brien, E. P. The Driving Force for Co-translational Protein Folding is Weaker In the Ribosome Vestibule due to Greater Water Ordering. Chemical Science 2021. 12, 11851-11857. Read more\nNissley, D. A.; Vu, Q. V.; Trovato, F.; Ahmed, N.; Jiang, Y.; Li, M. S.; O’Brien, E. P. Electrostatic Interactions Govern Extreme Nascent Protein Ejection Times from Ribosomes and Can Delay Ribosome Recycling. J. Am. Chem. Soc. 142, 13, 6103-6110 (2020). Read more"
  },
  {
    "objectID": "pages/about/index.html",
    "href": "pages/about/index.html",
    "title": "About me",
    "section": "",
    "text": "Quyen V. Vu\nPhD student at IFPAN, Poland (vuqv.phys@gmail.com).\nI am nowhere and I am everywhere.\nFull CV (PDF)\n\nFavourite quote:\nWe all fear what we do not understand.\n― Dan Brown, The Lost Symbol\n\n\n\nBrief CV:\n\n\n\n\nPeriod\n\n\nDegree\n\n\nWhere\n\n\nSupervisor(s)\n\n\n\n\n\n\n2018-2023\n\n\nPhD in Physics (Computational Biophysics)\n\n\nInstitute of Physics, Polish Academy of Sciences  Thesis\n\n\nProf. Edward P. O'Brien, and  Prof. Mai Suan Li\n\n\n\n\n2015-2017\n\n\nMsc in Physics\n\n\nVietnam National University-University of Science\n\n\nProf. Toan T. Nguyen\n\n\n\n\n2011-2015\n\n\nBsc in Physics\n\n\nVietnam National University-University of Science  (Talented Program of Physics)\n\n\nProf. Toan T. Nguyen"
  },
  {
    "objectID": "pages/team/index.html",
    "href": "pages/team/index.html",
    "title": "Team",
    "section": "",
    "text": "Avatar\n\n\nName\n\n\nDescriptions\n\n\nID\n\n\n\n\n\n\n\n\n\nQuyen Vu\n\n\nNice, friendly.\n\n\nM-0\n\n\n\n\n\n\n\nQuyen Vu\n\n\nYes- still me but with a toxic demeanor, unpredictable.\n\n\nM-2015"
  },
  {
    "objectID": "posts/python/Problems-regarding-ipywidgets.html",
    "href": "posts/python/Problems-regarding-ipywidgets.html",
    "title": "Problems regarding ipywidgets has no attribute ‘version_info’",
    "section": "",
    "text": "With the recent update of ipywidgets, Pyemma, MDAnalysis and some other package encounter problems with ipywidgets.\nThe dirty solution is install version ipywidgets&lt;8. version 7.7 may help and wait till developers of ipywidgets fix it."
  },
  {
    "objectID": "posts/python/conda-commands.html",
    "href": "posts/python/conda-commands.html",
    "title": "Some Conda Commands Frequently Used",
    "section": "",
    "text": "conda is a great python package environment."
  },
  {
    "objectID": "posts/python/conda-commands.html#post-under-construction",
    "href": "posts/python/conda-commands.html#post-under-construction",
    "title": "Some Conda Commands Frequently Used",
    "section": "(Post under construction)",
    "text": "(Post under construction)\n\nCreate a new environment: conda create -n ENV_NAME python=PYTHON_VERSION\nList all env: conda env list\nDelete environment: conda env remove -n ENV_NAME\nList all packages in the current env to text file so that can create new env later: conda list -e &gt; requirements.txt\nCreate new env from text file generated in step (4): conda create --name &lt;ENV_NAME&gt; --file requirements.txt"
  },
  {
    "objectID": "posts/linux-tips/linux_tips.html",
    "href": "posts/linux-tips/linux_tips.html",
    "title": "Some Linux commands frequently used",
    "section": "",
    "text": "This post contains some bash code-snippet to make my daily work easier.\n\nLoop through array\nIn case of looping through an continous list of number:\n#!/bin/bash\nfor i in {1..100}\ndo\n    # DO SOMETHING\ndone\nIf the list to loop is not continous, used the following form:\n#!/bin/bash\narr=( 3 4 9 10)\nfor i in \"${arr[@]}\"\ndo\n    echo $i       \n    #DONE SOMETHING HERE\ndone\nNote that arr definition is without whitespace.\n\n\nCounting number of lines in file\nSometime, after perform a long analysis of many simulations. We want to make sure if the analysis is done automatically, instead of looking into single file one-by-one, we can use bash script to do so.\n#!/bin/bash\nfor i in {1..100}\ndo\n        #echo \"$i\"\n        nl=$(wc -l &lt; $i/G_traj_${i}.dat)\n        if [ $nl -eq 400008 ]; then\n                echo \"traj $i :  $nl --- DONE\"\n        else\n                echo \"traj $i :  $nl --- Has not been DONE\"\n        fi\n\ndone\nIn this example, we loop through all simulations (1 to 100) and use command: wc -l &lt; $i/G_traj_${i}.dat to count the number of lines in file. we assign the output from this command to nl variable by packing previous command by $(command)\n\n\nSearch and replace in file\n#!/bin/bash\ncur_dir=`pwd`\nfor i in {1..100}\ndo\n        if [ -f \"/net/people/plgqvuvan/plggligroup/qvuvan/10proj/synthesize/cat3/wt/$i/traj/1/prot_l213_dissociation_final.cor\" ]; then\n                echo \"copy cor and vel file for run $i\"\n                rm -rf $i && mkdir $i\n                cp /net/people/plgqvuvan/plggligroup/qvuvan/10proj/synthesize/cat3/wt/$i/traj/1/prot_l213_dissociation_final.cor $i/\n                cp /net/people/plgqvuvan/plggligroup/qvuvan/10proj/synthesize/cat3/wt/$i/traj/1/prot_l213_dissociation_final.vel $i/\n                cp template/* $i/\n                sed \"s/SETINDEX/${i}/g\" -i ${i}/job_plgrid_gpu.sh\n                sed \"s/SETINDEX/${i}/g\" -i ${i}/control.cntrl\n        else\n                echo \"simulation $i is not finished\"\n        fi\ndone\n\nline 5: check if file prot_l213_dissociation_final.cor in specified directory existed. If yes, then run the code block.\nline 11-12: sed command used to search and replace pattern SETINDEX in file ${i}/job_plgrid_gpu.sh by value of variable $i\n\n\n\nSync data between local and cluster computer\n#!/bin/bash\nrsync -avzhP plgqvuvan@pro.cyfronet.pl:/net/people/plgqvuvan/plggligroup/qvuvan/10proj/post_trans/cat3/stride_by5/combine_trajs/ post_trans/\nThe above command sync the data from cluster pro.cyfronet.pl to local computer. To exclude files (e.g xtc file), add this: –exclude=*.xtc"
  },
  {
    "objectID": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html",
    "href": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html",
    "title": "Extracting information from IDP database and IDP classification",
    "section": "",
    "text": "Proteins are complex biomolecules crucial for the functioning of living organisms. They come in various shapes and sizes, each designed for specific functions within cells. Some proteins are well-structured, while others exhibit intrinsic disorder. Understanding the level of disorder in a protein is essential for unraveling its biological role. In this blog post, we’ll explore how we can determine if a protein is fully intrinsically disordered, contains structured regions with disordered loops, or is entirely structured, using insights from Necci et al.’s 2016 research (Necci, Piovesan, and Tosatto (2016)). Given a protein sequence and structural ensemble, how do we know if it is fully IDP, protein with some IDR regions or it is fully structured with some loops?\n\n\nNecci and colleagues proposed a classification scheme for intrinsically disordered proteins (IDPs) based on the number of consecutive disordered residues:\n\nShort IDR: Proteins with \\(5-19\\) consecutive disordered residues.\nLong IDR: Proteins with \\(\\ge 20\\) consecutive disordered residues.\nFully Disordered Protein: Proteins with \\(\\ge 50\\) consecutive disordered residues and more than 95% of their content being disordered.\n\nThis classification scheme provides a straightforward way to categorize proteins based on their disorder characteristics, offering valuable insights into their potential functions.\n\n\n\nTo determine a protein’s disorder characteristics, we need to map sequence features from two primary resources: DISPROT and MobiDB. DISPROT is a database that provides information about where structured and disordered regions are located in a protein sequence, while MobiDB offers valuable data on protein mobility and disorder.\nUsing these resources, we can efficiently evaluate the disorder characteristics of a given protein entry in the PED (Protein Ensemble Database). By cross-referencing the information from DISPROT and MobiDB with the PED entries, we can gain a comprehensive understanding of a protein’s structural properties."
  },
  {
    "objectID": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#classifying-proteins-based-on-disorder",
    "href": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#classifying-proteins-based-on-disorder",
    "title": "Extracting information from IDP database and IDP classification",
    "section": "",
    "text": "Necci and colleagues proposed a classification scheme for intrinsically disordered proteins (IDPs) based on the number of consecutive disordered residues:\n\nShort IDR: Proteins with \\(5-19\\) consecutive disordered residues.\nLong IDR: Proteins with \\(\\ge 20\\) consecutive disordered residues.\nFully Disordered Protein: Proteins with \\(\\ge 50\\) consecutive disordered residues and more than 95% of their content being disordered.\n\nThis classification scheme provides a straightforward way to categorize proteins based on their disorder characteristics, offering valuable insights into their potential functions."
  },
  {
    "objectID": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#mapping-sequence-features",
    "href": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#mapping-sequence-features",
    "title": "Extracting information from IDP database and IDP classification",
    "section": "",
    "text": "To determine a protein’s disorder characteristics, we need to map sequence features from two primary resources: DISPROT and MobiDB. DISPROT is a database that provides information about where structured and disordered regions are located in a protein sequence, while MobiDB offers valuable data on protein mobility and disorder.\nUsing these resources, we can efficiently evaluate the disorder characteristics of a given protein entry in the PED (Protein Ensemble Database). By cross-referencing the information from DISPROT and MobiDB with the PED entries, we can gain a comprehensive understanding of a protein’s structural properties."
  },
  {
    "objectID": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#import-libraries",
    "href": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#import-libraries",
    "title": "Extracting information from IDP database and IDP classification",
    "section": "Import libraries",
    "text": "Import libraries\n\n# import library\nimport json\nimport pandas as pd\nimport requests\n\nfrom colorama import Fore, Back, Style\nfrom itertools import groupby\nfrom operator import itemgetter"
  },
  {
    "objectID": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#retrieve-disorder-information-from-mobidb",
    "href": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#retrieve-disorder-information-from-mobidb",
    "title": "Extracting information from IDP database and IDP classification",
    "section": "Retrieve disorder information from MobiDB",
    "text": "Retrieve disorder information from MobiDB\nNext, we define a function to retrieve disorder-related information for a protein using the DISPROT/MobiDB database.\nHere, we only use data with highest confidence from MobiDB. All highest confidence sources of evidence are treated equally.\nSpecifically, the following categories will be used:\n\ncurated-disorder-priority: Residues are annotated with the source with the highest of evidence from curated data\nhomology-disorder-priority: Residues are annotated with the source with the highest of evidence from homology (see above for more information)\nderived-missing_residues-priority: Residues are annotated when at least 90% of the children are annotated\nprediction-disorder-priority: Residues are annotated with the source with the highest of evidence from prediction (see above for more information)\nderived-mobile-th_90: Mobile residues, residues are annotated when at least 90% of the children are annotated\n\n\ndef get_mobidb_disordered_data(uniprot):\n    \"\"\"\n    Retrieve disorder-related information for a protein using the DISPROT/MobiDB database.\n\n    This function takes a UniProt ID as input and queries the DISPROT/MobiDB database to retrieve disorder-related\n    information for the specified protein. It returns a list of intervals representing disordered regions based on\n    specific keywords, with a preference for \"curated\" disorder information.\n\n    Args:\n        uniprot (str): The UniProt ID of the protein for which disorder information is to be retrieved.\n\n    Returns:\n        list: A list of intervals representing disordered regions. Each interval is represented as a tuple (start, end).\n\n    Example:\n        disordered_regions = get_mobidb_disordered_data(\"P12345\")\n        print(\"Disordered Regions:\")\n        for interval in disordered_regions:\n            print(f\"[{interval[0]}, {interval[1]}]\")\n\n    Note:\n        - The function queries the DISPROT/MobiDB database via its API.\n        - It prioritizes \"curated\" disorder information when available.\n        - Disorder information is returned based on specific keywords.\n        - If no disorder information is found, an empty list is returned.\n    \"\"\"\n    # The information about these triplets can be found here: https://mobidb.org/help#vocabulary\n    keywords = ['curated-disorder-priority',\n                'homology-disorder-priority',\n                'derived-missing_residues-priority', \n                'prediction-disorder-priority', 'derived-mobile-th_90'] #, , 'derived-mobile_context_dependent-th_90'\n\n    url = 'https://mobidb.org/api/download?format=json&acc=' + uniprot\n\n    # Check if the ID exists in DISPROT/MOBIDB\n    res = requests.get(url)\n\n    if res.status_code == 200:\n        try:\n            result = res.json()\n        except:\n            print(\"ID DOES NOT EXITS IN THE DATABASE\")\n            return []  # Return an empty list if JSON parsing fails\n\n        disordered_regions = []\n\n        for key in keywords:\n            if key in result.keys():\n                regions = result[key]['regions']\n                # print(key, regions)\n                disordered_regions.append(tuple(regions))\n\n        # print(disordered_regions)\n        if len(disordered_regions) == 0:\n            print(\"NO DISORDER REGION FOUND\")\n            return \"Fully structured\"\n        return disordered_regions\n\n    \n    return []  # Return an empty list if the ID does not exist in the database\n\n\n# Example:\nget_mobidb_disordered_data(\"P02751\")\n\n[([1, 47], [272, 296], [2083, 2198], [2432, 2477]),\n ([153, 166], [702, 730], [2330, 2339])]\n\n\nAs we can see, here the function only extracts disorder regions from various criterias and not doing anything. To be more clear, we need to sort out, combine continous regions in a compact form for more intuitative."
  },
  {
    "objectID": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#merge-data-in-compact-continuous-region-form",
    "href": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#merge-data-in-compact-continuous-region-form",
    "title": "Extracting information from IDP database and IDP classification",
    "section": "Merge data in compact, continuous region form",
    "text": "Merge data in compact, continuous region form\nAs an example, the get_mobidb_disordered_data prints all regions without sorting. This is quite messy. We will define a merge_data function to sort, combine the results and print in compact form:\n\ndef merge_data(data_sets):\n    \"\"\"\n    Merge and compact intervals from multiple data sets.\n\n    This function takes a list of data sets, where each data set consists of one or more intervals.\n    It combines intervals from all data sets, merges overlapping intervals, and further merges adjacent intervals.\n    The resulting merged intervals are printed in compact form.\n\n    Args:\n        data_sets (list): A list of data sets, where each data set is represented as a tuple of intervals.\n            Each interval is represented as a list with two elements: [start, end].\n\n    Returns:\n        None: The function prints the merged intervals but does not return a value.\n\n    Example:\n        data_sets = [([1, 56],), ([1, 56],), ([59, 68],), ([57, 58], [150, 160])]\n        merge_data(data_sets)\n\n    Output:\n        Compact Merged Intervals:\n        [[1, 68], [150, 160]]\n\n    Note:\n        - Intervals within each data set are merged if they overlap.\n        - Intervals across different data sets are combined and merged.\n        - Adjacent intervals are further merged into compact intervals.\n\n    \"\"\"\n    # print(data_sets)\n    if isinstance(data_sets, str):\n        # if protein is fully structure, data_sets will be string- then do nothing\n        return []\n    if len(data_sets) == 0:\n        # print(\"[ID does not exist in Database]\")\n        return []\n    # Initialize an empty list to store all intervals\n    all_intervals = []\n\n    # Iterate through each data set and collect all intervals\n    for intervals in data_sets:\n        all_intervals.extend(intervals)\n\n    # Sort the intervals by their start values\n    all_intervals.sort(key=lambda x: x[0])\n\n    # Initialize a list to store the merged intervals\n    merged_intervals = []\n\n    # Iterate through the sorted intervals and merge overlapping intervals\n    for interval in all_intervals:\n        if not merged_intervals or interval[0] &gt; merged_intervals[-1][1]:\n            # If the interval does not overlap with the last merged interval, add it as a new merged interval\n            merged_intervals.append(interval)\n        else:\n            # If the interval overlaps with the last merged interval, merge them\n            merged_intervals[-1] = [merged_intervals[-1][0], max(merged_intervals[-1][1], interval[1])]\n\n    # Further merge adjacent intervals\n    final_merged_intervals = []\n    current_interval = merged_intervals[0]\n\n    for interval in merged_intervals[1:]:\n        if current_interval[1] + 1 == interval[0]:\n            current_interval[1] = interval[1]\n        else:\n            final_merged_intervals.append(current_interval)\n            current_interval = interval\n\n    final_merged_intervals.append(current_interval)\n\n    # Print the merged intervals in compact form\n    # print(\"Disorder regions:\")\n    # print(tuple(final_merged_intervals))\n    return tuple(final_merged_intervals)"
  },
  {
    "objectID": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#count-how-many-disorder-residues-in-a-given-sequence",
    "href": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#count-how-many-disorder-residues-in-a-given-sequence",
    "title": "Extracting information from IDP database and IDP classification",
    "section": "Count how many disorder residues in a given sequence",
    "text": "Count how many disorder residues in a given sequence\nAlright, Now we have the information about where is the disorder regions of full sequence (MobiDB) and where is the sequence in PED mapping to MobiDB. Question is how many residues in PED sequence are disorder? For that purpose, we define a count_overlap function to do this job.\nThis function takes two arguments (as docs of function below)\nfirst_range is the region of PED sequence, as a tuple\nsecond_ranges is the disorder regions from MobiDB, is a tuple or list of tuples in case sequence contains many disorder regions and not continous.\n\ndef count_overlap(first_range, second_ranges):\n    \"\"\"\n    Count the number of overlapping numbers between two range tuples.\n\n    Args:\n        first_range (tuple): A tuple representing the first range as (start, end).\n        second_ranges (tuple or list of tuples): A tuple or list of tuples representing the second ranges.\n\n    Returns:\n        list: A list of counts, where each count corresponds to the number of overlapping numbers for each region\n            in the second_ranges.\n\n    Example:\n        first_range = (57, 160)\n        second_ranges = ([1, 68], [150, 160])\n        overlaps = count_overlap(first_range, second_ranges)\n        print(overlaps)  # Output: [12, 11]\n\n    \"\"\"\n    # print(first_range, second_ranges)\n    if isinstance(second_ranges, tuple):\n        second_ranges = list(second_ranges)\n    # print(second_ranges)\n    overlaps = []\n\n    if not bool(second_ranges):\n        # when disorder region is empty-fully structure-return an empty list\n        return []\n    \n    for second_range in second_ranges:\n        # print(second_range)\n        start = max(first_range[0], second_range[0])\n        end = min(first_range[1], second_range[1])\n\n        if start &lt;= end:\n            overlap_count = end - start + 1\n            overlaps.append(overlap_count)\n        else:\n            overlaps.append(0)\n\n    return overlaps\n\n\n\n# Example usage:\nfirst_range = (57, 160)\nsecond_ranges = ([1, 68],)#, [150, 160]\noverlaps = count_overlap(first_range, second_ranges)\nprint(overlaps)\n\n[12]"
  },
  {
    "objectID": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#classify-protein-based-on-the-length-of-disorder-regions",
    "href": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#classify-protein-based-on-the-length-of-disorder-regions",
    "title": "Extracting information from IDP database and IDP classification",
    "section": "Classify protein based on the length of disorder regions",
    "text": "Classify protein based on the length of disorder regions\nWe have the length of IDR regions, here we define a function to classify proteins into catagories as stated in the introduction:\n\nFully IDP\nLong IDR\nShort IDR\nFully Structured\n\n\ndef classify_idr_regions(length_of_idr, length_in_ped):\n    \"\"\"\n    Classifies Intrinsic Disorder Regions (IDR) based on specified criteria.\n\n    Args:\n        length_of_idr (list of int): A list of integer values representing the lengths\n            of individual IDR regions.\n        length_of_pdb (int): The total length of the Protein Data Bank (PDB) structure.\n\n    Returns:\n        str: A classification string for the IDR regions based on the following criteria:\n            - If any region has a length greater than or equal to 50 and accounts for\n              more than 95% of the PDB length, it is classified as \"Full IDP.\"\n            - If any region has a length greater than or equal to 20, it is classified as\n              \"Long IDR.\"\n            - If any region has a length between 5 and 19 (inclusive), it is classified as\n              \"Short IDR.\"\n            - If all regions have lengths less than 5, the entire structure is classified\n              as \"Fully Structured.\"\n\n    Examples:\n        &gt;&gt;&gt; length_of_idr = [1, 10, 69]\n        &gt;&gt;&gt; length_of_pdb = 69\n        &gt;&gt;&gt; classification = classify_idr_regions(length_of_idr, leng_of_pdb)\n        &gt;&gt;&gt; print(classification)\n        \"Long IDR\"\n        \n    Note:\n        - If multiple criteria apply to different regions, the most strict criteria\n          are applied. The priority is \"Full IDP\" &gt; \"Long IDR\" &gt; \"Short IDR\" &gt; \"Fully Structured.\"\n        - This function assumes that the input values are valid and correctly represent\n          the lengths of IDR regions and the PDB length.\n\n    \"\"\"\n    if length_in_ped &lt; 20:\n        return \"Not Classified\"\n    \n    classifications = []\n\n    for idr_length in length_of_idr:\n        if idr_length &gt;= 50 and (idr_length / length_in_ped) &gt;= 0.95:\n            # the second condition should be sum(length_of_idr)/ length_in_ped it will make more sense\n            classifications.append(\"Full IDP\")\n        elif idr_length &gt;= 20:\n            classifications.append(\"Long IDR\")\n        elif 5 &lt;= idr_length &lt;= 19:\n            classifications.append(\"Short IDR\")\n        else:\n            classifications.append(\"Undefined\")\n\n    # Determine the final classification based on priority\n    if \"Full IDP\" in classifications:\n        return \"Fully IDP\"\n    elif \"Long IDR\" in classifications:\n        return \"Long IDR\"\n    elif \"Short IDR\" in classifications:\n        return \"Short IDR\"\n    else:\n        return \"Fully Structured\"\n\n# Example usage:\n# length_of_idr = [3, 4]\n# length_in_ped = 18\n# classification = classify_idr_regions(length_of_idr, length_in_ped)\n# print(\"Classification:\", classification)"
  },
  {
    "objectID": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#combine-all-together",
    "href": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#combine-all-together",
    "title": "Extracting information from IDP database and IDP classification",
    "section": "Combine all together:",
    "text": "Combine all together:\nFunction to do the main task: get information from PED, pass uniprot to call MobiDB then do all other stuffs:\n\n\"\"\"\nThis function work very well. need some modifies for printing.\n\"\"\"\ndef get_ped_stats(PEDID):\n    \"\"\"\n    Retrieve and display protein ensemble deposition (PED) statistics for a given PED entry.\n\n    This function queries the Protein Ensemble Deposition (PED) API to retrieve statistics and information for a\n    specific PED entry identified by its PEDID. It provides details such as the entry ID, title, and information\n    about chains and fragments within the entry. Additionally, it calculates the number of overlapping residues\n    between fragment positions and disordered regions retrieved from the DISPROT/MobiDB database.\n\n    Args:\n        PEDID (str): The PED entry ID for the entry to retrieve statistics.\n\n    Returns:\n        None: The function prints the PED statistics and overlap counts but does not return a value.\n\n    Example:\n        get_ped_stats(\"PED12345\")\n\n    Note:\n        - This function requires the 'requests' library for HTTP requests and 'colorama' for colored output.\n        - It queries the PED API to obtain entry information.\n        - It retrieves disordered region information using the DISPROT/MobiDB database.\n        - Overlapping residues between fragment positions and disordered regions are calculated and displayed.\n    \"\"\"\n    url = \"https://deposition.proteinensemble.org/api/v1/entries/\" + PEDID\n    res = requests.get(url)\n    if res.status_code == 200:\n        res = res.json()\n        print(\"PED ID\\t# chains in entry\\tProtein name\\t\\\"Length in PED (tag counted)\\\"\\tUniProt\\tLength UniProt\\t\\\"Disordered region from MobiDB/DisProt\\\"\\t\\\"PDB region (align to Uniprot)\\\"\\tLength of IDR\\t Classification\")\n\n        construct_chains = res['construct_chains']\n\n        for chain in construct_chains:\n            if len(construct_chains) == 1:\n                # chain_name = chain['chain_name']\n                entry = PEDID\n            else:\n                # chain_name = res['entry_id'] + '_' + chain['chain_name']\n                entry= PEDID + '_' + chain['chain_name']\n\n            n_fragments = len(chain['fragments'])\n            fragments = chain['fragments']\n\n            for fragment, fragment_stats in zip(fragments, chain['fragments_stats']):\n                protein_name = fragment['description']\n                length_in_ped = fragment_stats['length_total_pdb']\n                uniprot = fragment_stats['uniprot']\n                length_uniprot = fragment_stats['length_total_uniprot']\n\n                mobi_disorder_regions = tuple()\n                if fragment_stats['uniprot'] is not None:\n                    mobi_disorder_regions = merge_data(get_mobidb_disordered_data(fragment_stats['uniprot']))\n\n                pdb_region = tuple([fragment['start_position'], fragment['end_position']])\n                length_of_idr = count_overlap(pdb_region, mobi_disorder_regions)\n                classification = classify_idr_regions(length_of_idr, length_in_ped)\n\n                print(f\"{entry}\\t{len(construct_chains)}\\t{protein_name}\\t{length_in_ped}\\t{uniprot}\\t{length_uniprot}\\t{mobi_disorder_regions}\\t{pdb_region}\\t{length_of_idr}\\t{classification}\")\n    elif res.status_code == 404:\n        print(f\"Entry {PEDID} does not exist\")\n        return"
  },
  {
    "objectID": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#explanation-of-the-example",
    "href": "posts/2023/October/Extracting_disorder_regions_from_MobiDB.html#explanation-of-the-example",
    "title": "Extracting information from IDP database and IDP classification",
    "section": "Explanation of the example",
    "text": "Explanation of the example\nHere, we are looking at the PED entry PED00019.\nThis entry comprises two chains: Chain A and Chain B. To enhance clarity, we opt to represent the name as ENTRY+CHAIN for both.\nChain A consists of 160 residues and can be traced back to the UniProt ID O14558, which is also characterized by 160 residues.\nThe disorder regions, as identified by MobiDB, are situated at positions [1, 68] and [150, 160].\nIn the PDB data within PED, the entire sequence, spanning positions [1, 160], is cataloged.\nUpon a careful comparison of PED PDB data and the disorder regions, we observe the presence of two distinct disorder regions, one spanning 68 residues and the other 11 residues in length.\nSimilar explaination for chain B."
  },
  {
    "objectID": "posts/Running-jobs-on-clusters/running-jobs-on-cluster.html",
    "href": "posts/Running-jobs-on-clusters/running-jobs-on-cluster.html",
    "title": "Slurm scripts and commands to submit jobs on clusters",
    "section": "",
    "text": "#!/bin/bash\n#SBATCH --partition plgrid\n#SBATCH --job-name cryosparc-master\n#SBATCH --nodes 1\n#SBATCH --ntasks-per-node 1\n#SBATCH --mem 15GB\n#SBATCH --time 72:00:00\n#SBATCH -C localfs\n#SBATCH -A plgrisa-cpu\n#SBATCH --dependency=singleton\n#SBATCH --output cryosparc-master-log-%J.txt\n#SBATCH --signal=B:2@240\n\necho \"Job run\" &gt; test.txt\n\n\n\n#!/bin/bash\n#SBATCH --job-name ispe_0\n#SBATCH --nodes 1\n#SBATCH --partition plgrid-gpu-v100\n#SBATCH --gres=gpu:1\n#SBATCH --ntasks-per-node 1\n#SBATCH --mem 15GB\n#SBATCH --time 48:00:00\n#SBATCH -C localfs\n#SBATCH -A plgrisa-gpu\n#SBATCH --dependency=singleton\n#SBATCH --output=output.out\n#SBATCH --error=error.err\n#SBATCH --signal=B:2@240\n## ares\nconda init bash\nsource /net/people/plgrid/plgqvuvan/plggligroup/qvv5013/anaconda3/etc/profile.d/conda.sh\nconda activate py310\ncd $SLURM_SUBMIT_DIR\necho `pwd`\n#\npython single_run.py -f control.cntrl\n\n\n\nsrun -p plgrid -N 1 --ntasks-per-node=8 -n 8 --time=0-8 -A plgrisa-cpu --pty /bin/bash -l\nTo request for 1 node (-N 1), 8 threads and 8hrs"
  },
  {
    "objectID": "posts/Running-jobs-on-clusters/running-jobs-on-cluster.html#interactive-mode-on-ares",
    "href": "posts/Running-jobs-on-clusters/running-jobs-on-cluster.html#interactive-mode-on-ares",
    "title": "Slurm scripts and commands to submit jobs on clusters",
    "section": "",
    "text": "srun -p plgrid -N 1 --ntasks-per-node=8 -n 8 --time=0-8 -A plgrisa-cpu --pty /bin/bash -l\nTo request for 1 node (-N 1), 8 threads and 8hrs"
  },
  {
    "objectID": "posts/Research/All-Atom_simulations_IDPs.html",
    "href": "posts/Research/All-Atom_simulations_IDPs.html",
    "title": "All-Atom simulations of IDP",
    "section": "",
    "text": "List of simulation works using all-atom resolution and force field used:\n\nGalvanetto et al. (2023) used Amber99SBws force field with the TIP4P/2005s water model. Their FFs for Gromacs can be download from: https://github.com/bestlab/force_fields. This folder contains various FFs with an explaination. I have made a fork from their repo for backup to my own account.\nMohanty et al. (2023) used AMBER99SBws-STQ force field with TIP4P/2005. Download from HERE. A backup version is located in: Research/IDP_AA_FFs/\n\n\n\n\n\n\nReferences\n\nGalvanetto, Nicola, Miloš T. Ivanović, Aritra Chowdhury, Andrea Sottini, Mark F. Nüesch, Daniel Nettels, Robert B. Best, and Benjamin Schuler. 2023. “Extreme dynamics in a biomolecular condensate.” Nature 25 (February). https://doi.org/10.1038/s41586-023-06329-5.\n\n\nMohanty, Priyesh, Jayakrishna Shenoy, Azamat Rizuan, José F. Mercado-Ortiz, Nicolas L. Fawzi, and Jeetain Mittal. 2023. “A Synergy Between Site-Specific and Transient Interactions Drives the Phase Separation of a Disordered, Low-Complexity Domain.” Proceedings of the National Academy of Sciences 120 (34): e2305625120. https://doi.org/10.1073/pnas.2305625120."
  },
  {
    "objectID": "posts/openMM/mpipi_model.html",
    "href": "posts/openMM/mpipi_model.html",
    "title": "Implementation of Mpipi model in OpenMM",
    "section": "",
    "text": "Mpipi model is a coarse-grain model for simulating IDP. The original code is implemented in LAMMPS but I am not familiar with LAMMPS either.\nThe original paper: Physics-driven coarse-grained model for biomolecular phase separation with near-quantitative accuracy. Nature Computational Science volume 1, pages 732–743 (2021). PDF file can be accessed from here\nThis model is different from other hps-based model is instead of using LJ12-6 potential, it used Wang-Frenkel potential.\n\\(\\phi_{i,j}(r) = \\epsilon_{i,j} \\alpha_{ij} \\left[\\left(\\frac{\\sigma_{ij}}{r}\\right)^{2\\mu_{ij}}-1 \\right] \\left[ \\left(\\frac{R_{ij}}{r}\\right)^{2\\mu_{ij}}-1\\right]^{2\\nu_{ij}}\\)\nwhere,\n\\(\\alpha_{ij}= 2\\nu_{ij} \\times \\left(\\frac{R_{ij}}{\\sigma_{ij}}\\right)^{2\\mu_{ij}} \\times \\left[ \\frac{(2\\nu_{ij}+1)}{2\\nu_{ij} \\left( \\left(\\frac{R_{ij}}{\\sigma_{ij}}\\right)^{2\\mu_{ij}}-1 \\right) })\\right]^{2\\nu_{ij}+1}\\)\nThey used: \\(\\nu_{ij}=1\\) and \\(R_{ij}=3\\sigma_{ij}\\)\n\\(\\sigma_{ij}, \\epsilon_{ij}, \\mu_{ij}\\) are parameters specified for each pair of interacting beads.\n\nImplementation\nTo implement this functional form. We used TabulatedFunction in OpenMM.\nFirst, we need to define the atom type of each residue, this definition is implemented in hps/parameters/model_parameters.py file as a dictionary, or github: here\n\nFunction is defined in hps/core/system.py. Here is the copied function:\n    def add_Wang_Frenkel_Forces(self, use_pbc: bool):\n        \"\"\"\n        MPIPI model. using TabulatedFunction for pair interaction.\n        More information about TabulatedFUnction can be found here:\n        http://docs.openmm.org/7.2.0/api-c++/generated/OpenMM.Discrete2DFunction.html\n        \"\"\"\n        wang_frenkel_cutoff = 2.5 * unit.nanometer\n\n        \"\"\"\n        In the model module, we only call this function when the model is mpipi so the following condition likely to be\n        true. But to be sure, we still check here.\n        \"\"\"\n        assert self.model in ['mpipi'], \"Wang-Frenkel is only used in Mpipi model.\"\n\n        table_eps = model_parameters.parameters[self.model]['eps_ij']\n        table_eps_ravel = table_eps.ravel().tolist()\n\n        table_sigma = model_parameters.parameters[self.model]['sigma_ij']\n        table_sigma_ravel = table_sigma.ravel().tolist()\n\n        table_nu = model_parameters.parameters[self.model]['nu_ij']\n        table_nu_ravel = table_nu.ravel().tolist()\n\n        table_mu = model_parameters.parameters[self.model]['mu_ij']\n        table_mu_ravel = table_mu.ravel().tolist()\n\n        table_rc = model_parameters.parameters[self.model]['rc_ij']\n        table_rc_ravel = table_rc.ravel().tolist()\n\n        # number of atom types in model. currently with protein, there are 20.\n        n_atom_types = table_sigma.shape[0]\n\n        # eps, sigma, nu, mu, rc: load from tabular table\n        \"\"\"\n        Note: here we use abs function in ((rc/r)^(2*mu)-1)^(2*nu) because otherwise, nu added by parameters is float.\n        when r&gt;rc, produces this is negative and non-integer power of float is nan.\n        \"\"\"\n        energy_function = 'eps * 2*nu*(rc/sigma)^(2*mu) * ((2*nu+1)/(2*nu*((rc/sigma)^(2*mu)-1)))^(2*nu+1)'\n        energy_function += '* ((sigma/r)^(2*mu)-1 )* abs((rc/r)^(2*mu)-1)^(2*nu);'\n        energy_function += 'eps = eps_table(id1, id2); sigma = sigma_table(id1, id2);'\n        energy_function += 'nu = nu_table(id1, id2);'\n        energy_function += 'mu = mu_table(id1, id2);'\n        energy_function += 'rc=rc_table(id1, id2)'\n\n        self.wang_Frenkel_Force = mm.CustomNonbondedForce(energy_function)\n        self.wang_Frenkel_Force.addTabulatedFunction('eps_table', mm.Discrete2DFunction(n_atom_types, n_atom_types,\n                                                                                        table_eps_ravel))\n        self.wang_Frenkel_Force.addTabulatedFunction('sigma_table',\n                                                     mm.Discrete2DFunction(n_atom_types, n_atom_types,\n                                                                           table_sigma_ravel))\n        self.wang_Frenkel_Force.addTabulatedFunction('nu_table', mm.Discrete2DFunction(n_atom_types, n_atom_types,\n                                                                                       table_nu_ravel))\n        self.wang_Frenkel_Force.addTabulatedFunction('mu_table', mm.Discrete2DFunction(n_atom_types, n_atom_types,\n                                                                                       table_mu_ravel))\n        self.wang_Frenkel_Force.addTabulatedFunction('rc_table', mm.Discrete2DFunction(n_atom_types, n_atom_types,\n                                                                                       table_rc_ravel))\n        self.wang_Frenkel_Force.addPerParticleParameter('id')\n\n        for i, atom in enumerate(self.atoms):\n            self.wang_Frenkel_Force.addParticle((self.particle_type_id[i],))\n\n        if use_pbc:\n            self.wang_Frenkel_Force.setNonbondedMethod(mm.NonbondedForce.CutoffPeriodic)\n        else:\n            self.wang_Frenkel_Force.setNonbondedMethod(mm.NonbondedForce.CutoffNonPeriodic)\n\n        self.wang_Frenkel_Force.setCutoffDistance(wang_frenkel_cutoff)\n\n        # set exclusion rule\n        bonded_exclusions = [(b[0].index, b[1].index) for b in list(self.topology.bonds())]\n        self.wang_Frenkel_Force.createExclusionsFromBonds(bonded_exclusions, self.bonded_exclusions_index)\n\nline 15-28: read the numpy array define \\(\\epsilon_{ij}\\), \\(\\nu_{ij}\\), \\(\\mu_{ij}\\), and \\(R_{ij}\\)\nline 38-43: define the energy function to pass into CustomNonbondedForce\nline 46-55: let openMM knows where to get eps_table and other tabulated variables\nthe only Per-Particle-Parameter is the id of the particle, this is id of the residue type which is defined in model_parameters.py\nOther lines are self-explained.\n\nThe above code used global cutoff for Wang-Frenkel potential is 2.5nm. As stated in the paper, authors used the cutoff for each pair interaction is \\(3\\sigma\\). To do so, we can add the stepsize function before the energy function. Step function in openMM work as: step(x)=0 if x&lt; 0 and 1 if x&gt;=0.\nWe can add: step(rc-r)*eps... to the energy function. So, if \\(r \\gt rc \\Rightarrow rc-r \\lt 0 \\Rightarrow step(rc-r) =0\\)\n\n\nTabulatedFunction:\nThis is an interesting point from computational perpective.\nMore information about TabulatedFunction can be found in OpenMM documentation here\nHere, I will use 2D Discrete function.\nThe function works as follow: Discrete2DFunction(xsize, ysize, val)\nthe tabulated values of the function f(x,y), ordered so that values[i+xsize*j] = f(i,j). This must be of length xsize*ysize.\nThis is the reason why I use ravel() function from python to covnert 2D array to 1D array above.\nIn this code, I call:\n\\(\\epsilon_{ij}\\) = eps,\n\\(\\nu_{ij}\\) = nu,\n\\(\\mu_{ij}\\) = mu,\n\\(R_{ij}\\) = rc\nEach particle has Per-Particle-Parameter: id\nHere, we define, i.e: eps is a tabulated function: eps = eps_table(id1, id2). The value will be extracted from particle id\nIn my computer, the last part of the energy function: ((rc/r)^(2*mu)-1)^(2*nu) will be nan when r&gt;rc. This is very strange since nu is loaded as float but 2*nu will always be an even number. My computer doesn’t know that by some rounding rules of floating point. when r&gt;rc: rc/r&lt;1, rc/r-1&lt;0, and non-integer number power of negative value is NaN.\nThat’s why I used the abs function before this part as the power is even so they are equivalent."
  }
]